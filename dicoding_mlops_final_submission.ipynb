{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DAkk-qztTC2V",
        "outputId": "8ab6ea63-b83f-4c27-f022-0888c1e9afd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740542   nanos: 596634149 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740542   nanos: 606190681 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740573   nanos: 486353874 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_8\" transform_id: \"WriteSplit[train]/Write/Write/WriteImpl/WriteBundles\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740587   nanos: 3697156 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740587   nanos: 12600898 } message: \"Discarding unparseable args: [\\'--direct_runner_use_stacked_bundle\\', \\'--pipeline_type_check\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740590   nanos: 877217054 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_25\" transform_id: \"TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-12\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
            "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
            "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
            "WARNING:absl:Tables initialized inside a tf.function  will be re-initialized on every invocation of the function. This  re-initialization can have significant impact on performance. Consider lifting  them out of the graph context using  `tf.init_scope`.: compute_and_apply_vocabulary_1/apply_vocab/text_file_init/InitializeTableFromTextFileV2\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740626   nanos: 670076131 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740626   nanos: 677542209 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724740635   nanos: 44027328 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_110\" transform_id: \"TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-11\" \n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:absl:Examples artifact does not have payload_format custom property. Falling back to FORMAT_TF_EXAMPLE\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/data/experimental/ops/readers.py:1086: parse_example_dataset (from tensorflow.python.data.experimental.ops.parsing_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map(tf.io.parse_example(...))` instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " gender_xf (InputLayer)      [(None, 3)]                  0         []                            \n",
            "                                                                                                  \n",
            " smoking_history_xf (InputL  [(None, 4)]                  0         []                            \n",
            " ayer)                                                                                            \n",
            "                                                                                                  \n",
            " age_xf (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " hypertension_xf (InputLaye  [(None, 1)]                  0         []                            \n",
            " r)                                                                                               \n",
            "                                                                                                  \n",
            " heart_disease_xf (InputLay  [(None, 1)]                  0         []                            \n",
            " er)                                                                                              \n",
            "                                                                                                  \n",
            " bmi_xf (InputLayer)         [(None, 1)]                  0         []                            \n",
            "                                                                                                  \n",
            " HbA1c_level_xf (InputLayer  [(None, 1)]                  0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " blood_glucose_level_xf (In  [(None, 1)]                  0         []                            \n",
            " putLayer)                                                                                        \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)   (None, 13)                   0         ['gender_xf[0][0]',           \n",
            "                                                                     'smoking_history_xf[0][0]',  \n",
            "                                                                     'age_xf[0][0]',              \n",
            "                                                                     'hypertension_xf[0][0]',     \n",
            "                                                                     'heart_disease_xf[0][0]',    \n",
            "                                                                     'bmi_xf[0][0]',              \n",
            "                                                                     'HbA1c_level_xf[0][0]',      \n",
            "                                                                     'blood_glucose_level_xf[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " dense (Dense)               (None, 256)                  3584      ['concatenate[0][0]']         \n",
            "                                                                                                  \n",
            " dense_1 (Dense)             (None, 64)                   16448     ['dense[0][0]']               \n",
            "                                                                                                  \n",
            " dense_2 (Dense)             (None, 16)                   1040      ['dense_1[0][0]']             \n",
            "                                                                                                  \n",
            " dense_3 (Dense)             (None, 1)                    17        ['dense_2[0][0]']             \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 21089 (82.38 KB)\n",
            "Trainable params: 21089 (82.38 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "5000/5000 [==============================] - 36s 7ms/step - loss: 0.1136 - binary_accuracy: 0.9609 - val_loss: 0.1009 - val_binary_accuracy: 0.9640\n",
            "Epoch 2/10\n",
            "5000/5000 [==============================] - 36s 7ms/step - loss: 0.0898 - binary_accuracy: 0.9688 - val_loss: 0.0869 - val_binary_accuracy: 0.9698\n",
            "Epoch 3/10\n",
            "5000/5000 [==============================] - 37s 7ms/step - loss: 0.0861 - binary_accuracy: 0.9699 - val_loss: 0.0837 - val_binary_accuracy: 0.9714\n",
            "Epoch 4/10\n",
            "5000/5000 [==============================] - 34s 7ms/step - loss: 0.0845 - binary_accuracy: 0.9705 - val_loss: 0.0840 - val_binary_accuracy: 0.9711\n",
            "Epoch 5/10\n",
            "5000/5000 [==============================] - 35s 7ms/step - loss: 0.0846 - binary_accuracy: 0.9704 - val_loss: 0.0834 - val_binary_accuracy: 0.9717\n",
            "Epoch 6/10\n",
            "5000/5000 [==============================] - 34s 7ms/step - loss: 0.0834 - binary_accuracy: 0.9708 - val_loss: 0.0845 - val_binary_accuracy: 0.9710\n",
            "Epoch 7/10\n",
            "5000/5000 [==============================] - 35s 7ms/step - loss: 0.0838 - binary_accuracy: 0.9709 - val_loss: 0.0833 - val_binary_accuracy: 0.9713\n",
            "Epoch 8/10\n",
            "5000/5000 [==============================] - 33s 7ms/step - loss: 0.0826 - binary_accuracy: 0.9713 - val_loss: 0.0844 - val_binary_accuracy: 0.9711\n",
            "Epoch 9/10\n",
            "5000/5000 [==============================] - 34s 7ms/step - loss: 0.0827 - binary_accuracy: 0.9712 - val_loss: 0.0828 - val_binary_accuracy: 0.9714\n",
            "Epoch 10/10\n",
            "5000/5000 [==============================] - 35s 7ms/step - loss: 0.0822 - binary_accuracy: 0.9715 - val_loss: 0.0827 - val_binary_accuracy: 0.9716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.options.pipeline_options:Discarding unparseable args: ['----direct_num_workers=0']\n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724741027   nanos: 122927904 } message: \"No semi_persistent_directory found: Functions defined in __main__ (interactive session) may fail.\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/runners/worker/sdk_worker_main.py:361\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724741027   nanos: 132532596 } message: \"Discarding unparseable args: [\\'--pipeline_type_check\\', \\'--direct_runner_use_stacked_bundle\\']\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/options/pipeline_options.py:372\" thread: \"MainThread\" \n",
            "WARNING:apache_beam.runners.portability.local_job_service:Worker: severity: WARN timestamp {   seconds: 1724741036   nanos: 24491310 } message: \"Couldn\\'t find python-snappy so the implementation of _TFRecordUtil._masked_crc32c is not as fast as it could be.\" instruction_id: \"bundle_231\" transform_id: \"ReadFromTFRecordToArrow[eval][0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/SDFBoundedSourceReader/ParDo(SDFBoundedSourceDoFn)/Process\" log_location: \"/usr/local/lib/python3.10/dist-packages/apache_beam/io/tfrecordio.py:59\" thread: \"Thread-13\" \n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow_model_analysis/writers/metrics_plots_and_validations_writer.py:112: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n",
            "WARNING:absl:ArtifactQuery.property_predicate is not supported.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "from typing import Text\n",
        "\n",
        "from absl import logging\n",
        "from tfx.orchestration import metadata, pipeline\n",
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
        "\n",
        "PIPELINE_NAME = \"diabetes-pipeline\"\n",
        "\n",
        "# Pipeline inputs\n",
        "DATA_ROOT = \"data\"\n",
        "TRANSFORM_MODULE_FILE = \"modules/transform.py\"\n",
        "TRAINER_MODULE_FILE = \"modules/trainer.py\"\n",
        "\n",
        "# Pipeline outputs\n",
        "OUTPUT_BASE = \"outputs\"\n",
        "\n",
        "serving_model_dir = os.path.join(OUTPUT_BASE, \"serving_model\")\n",
        "pipeline_root = os.path.join(OUTPUT_BASE, PIPELINE_NAME)\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")\n",
        "\n",
        "def init_local_pipeline(\n",
        "    components, pipeline_root: Text\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    logging.info(f\"Pipeline root set to: {pipeline_root}\")\n",
        "    beam_args = [\n",
        "        \"--direct_running_mode=multi_processing\", # Add a space here\n",
        "        \"----direct_num_workers=0\"\n",
        "    ]\n",
        "\n",
        "    return pipeline.Pipeline(\n",
        "        pipeline_name=PIPELINE_NAME,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_args\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "from modules.components import init_components\n",
        "\n",
        "components = init_components(\n",
        "    DATA_ROOT,\n",
        "    training_module=TRAINER_MODULE_FILE,\n",
        "    transform_module=TRANSFORM_MODULE_FILE,\n",
        "    training_steps=5000,\n",
        "    eval_steps=1000,\n",
        "    serving_model_dir=serving_model_dir,\n",
        ")\n",
        "\n",
        "pipeline = init_local_pipeline(components, pipeline_root)\n",
        "BeamDagRunner().run(pipeline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_DUodP4nJkS"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
